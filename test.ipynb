{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os, lib\n",
    "from dotenv import load_dotenv\n",
    "import typing\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code snippet reads data from Excel files and processes it. It collects system reliability data, dates, and a DataFrame containing specific columns.\n",
    "\n",
    "Explanation:\n",
    "1. The code initializes three empty lists: `sys`, `dat`, and `df`.\n",
    "2. It searches for Excel files recursively in the current working directory.\n",
    "3. For each file:\n",
    "   a. Attempts to read data from the sheet named \"System Reliability Data\" using the openpyxl engine.\n",
    "   b. Reads additional data from the same sheet, skipping the first 8 rows.\n",
    "   c. Appends values from specific columns to the `sys` and `dat` lists.\n",
    "   d. Extracts a DataFrame containing specific columns from the remaining data and appends it to the `df` list.\n",
    "4. Any exceptions encountered during file processing are caught and ignored.\n",
    "\n",
    "Note:\n",
    "- This code assumes that the Excel files contain sheets named \"System Reliability Data.\"\n",
    "- Adjust the sheet names and column indices as needed for your specific data.\n",
    "\"\"\"\n",
    "\n",
    "sys, dat, df = [], [], []\n",
    "\n",
    "for file in glob.glob(os.path.join(os.getcwd(), \"**\", \"*.xlsx\"), recursive=True):\n",
    "    try:\n",
    "        a = pd.read_excel(file, sheet_name=\"System Reliability Data\", engine='openpyxl')\n",
    "        b = pd.read_excel(file, sheet_name=\"System Reliability Data\", engine='openpyxl', skiprows=8)\n",
    "        sys.append(a[\"Unnamed: 2\"][1])\n",
    "        dat.append(a[\"Unnamed: 2\"][2])\n",
    "        df.append(pd.DataFrame(b.iloc[:,17:-1].iloc[1]).T)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code snippet concatenates and processes data from a DataFrame. It assigns specific columns and sorts the DataFrame by date.\n",
    "\n",
    "Explanation:\n",
    "1. The code concatenates DataFrames contained in the list `df`.\n",
    "2. Two new columns are created in the resulting DataFrame: \"Date\" and \"System.\"\n",
    "3. Duplicate rows based on the \"Date\" column are removed.\n",
    "4. The \"Date\" column is set as the index of the DataFrame.\n",
    "5. The DataFrame is sorted in ascending order based on the date.\n",
    "\n",
    "Note:\n",
    "- Adjust column names and operations according to your specific data.\n",
    "\"\"\"\n",
    "\n",
    "raw_data: pd.DataFrame = pd.concat([df[j] for j in range(len(df))])\n",
    "raw_data[\"Date\"], raw_data[\"System\"] = dat, sys\n",
    "raw_data.index = raw_data[\"Date\"]\n",
    "data_co = raw_data.drop_duplicates().sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "System\n",
       "DTE Energy Co.                      12\n",
       "Alpena Power Co.                    12\n",
       "Northern States Power Co. (Xcel)    12\n",
       "Consumers Energy Co.                12\n",
       "Upper Peninsula Power Co.           12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_co[\"System\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields: typing.Dict[int, str] = {\n",
    "    1: \"Total Customer outages\",\n",
    "    2: \"Total number of outages causes by equipment failure\",\n",
    "    3: \"Total number of outages caused by lightning\",\n",
    "    4: \"Total number of planned and forced outages\",\n",
    "    5: \"Total number of outages caused by transmission or generation failure\",\n",
    "    6: \"Total number of outages caused by an act of the public at large\",\n",
    "    7: \"Total number of outages caused by trees\",\n",
    "    8: \"Total number of outage caused by weather\", # This\n",
    "    9: \"Total number of outages caued by animal interference\",\n",
    "    10: \"Total number of outages caused by unknonwn causes\", # This\n",
    "    11: \"Total number of outages caused by other causes\", # This\n",
    "    12: \"System Average Interruption Duration Index\", # ! SAIDI\n",
    "    13: \"System Average Interruption Frequency Index\", # ! SAIFI\n",
    "    14: \"Customer Average Interruption Duration Index (contribution to total CAIDI)\", # ! CAIDI\n",
    "    15: \"Average Service Availability Index\",\n",
    "    16: \"NII-1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Total Customer outages' not found in data_co. Skipping.\n",
      "Column 'Total number of outages caused by transmission or generation failure' not found in data_co. Skipping.\n",
      "Column 'Customer Average Interruption Duration Index (contribution to total CAIDI)' not found in data_co. Skipping.\n",
      "Column 'NII-1' not found in data_co. Skipping.\n"
     ]
    }
   ],
   "source": [
    "for col_name in fields.values():  \n",
    "    try:\n",
    "        data_co[col_name.strip()] = data_co[col_name.strip()].astype(float).fillna(0)\n",
    "    except KeyError:\n",
    "        print(f\"Column '{col_name}' not found in data_co. Skipping.\")\n",
    "\n",
    "try:\n",
    "    data_co['Date'] = pd.to_datetime(data_co['Date']) \n",
    "except (KeyError, ValueError):  \n",
    "    print(\"'date' column not found or invalid format. Skipping conversion.\")\n",
    "\n",
    "data_co['NII-1'] = data_co[fields.get(8)] + data_co[fields.get(10)] + data_co[fields.get(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code snippet visualizes a specific reliability index for different energy systems. It normalizes the data and plots it for each system.\n",
    "\n",
    "Explanation:\n",
    "1. The dictionary `fields` maps field IDs to their corresponding descriptions.\n",
    "2. The variable `field_to_analyze` is set to the description of the reliability index to be analyzed (e.g., SAIFI).\n",
    "3. A plot is created for each energy system, showing the normalized values of the specified reliability index.\n",
    "4. The y-axis represents the normalized index values (scaled to a maximum of 1).\n",
    "\n",
    "Note:\n",
    "- Adjust the field descriptions and column names according to your specific data.\n",
    "\"\"\"\n",
    "\n",
    "field_to_analyze: str = fields.get(10)\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "for i in data_co[\"System\"].value_counts().index:\n",
    "    (data_co[data_co[\"System\"] == i][field_to_analyze] / (data_co[data_co[\"System\"] == i][field_to_analyze].max() or 1)).plot(label=i)\n",
    "    plt.legend(ncol=3)\n",
    "    plt.title(label=field_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_to_analyze: str = fields.get(16)\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "for date in data_co.index.unique():\n",
    "    print(date)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    raw_data = pd.read_csv(os.getenv('FILE_PATH_OMNI'), delimiter=\" +\", names=range(55), engine=\"python\")\n",
    "except:\n",
    "    print(\"<Exception> OMNI File required\")\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code snippet filters and processes data from a DataFrame. It selects specific columns based on their indices and restricts the data to a specific date range.\n",
    "\n",
    "Explanation:\n",
    "1. The DataFrame `raw_data` contains columns with indices 0, 1, 2, 38, 39, 40, 49, 50, 22, 23, 24, 27, 28, and 8.\n",
    "2. The minimum and maximum dates from the DataFrame `filtered_df` are determined.\n",
    "3. The years corresponding to these dates are extracted using `pd.Timestamp`.\n",
    "4. The data is filtered to include only rows where the year (column 0) falls within the specified date range.\n",
    "5. Column names are assigned to the processed data for better readability.\n",
    "\n",
    "Note:\n",
    "- Adjust column indices and date range according to your specific data.\n",
    "\"\"\"\n",
    "\n",
    "data_omni = raw_data[[0, 1, 2, 38, 39, 40, 49, 50, 22, 23, 24, 27, 28, 8]]\n",
    "min_date, max_date = data_co.index.min(), data_co.index.max()\n",
    "min_date_year, max_date_year = pd.Timestamp(min_date).year, pd.Timestamp(max_date).year\n",
    "filtered_data_omni = data_omni[(data_omni[0] >= min_date_year) & (data_omni[0] <= max_date_year)]\n",
    "\n",
    "filtered_data_omni.columns = [\n",
    "    \"Year\", # \n",
    "    \"DecimalYear\", #\n",
    "    \"Hour\", #\n",
    "    \"Kp\", #\n",
    "    \"R\", #\n",
    "    \"DST\", #\n",
    "    \"Ap\", #\n",
    "    \"F10.7\", #\n",
    "    \"Proton temperature\", #\n",
    "    \"Proton density\", #\n",
    "    \"Plasma speed\", #\n",
    "    \"Alpha/Proton ratio\", #\n",
    "    \"Flow Pressure\", #\n",
    "    \"Field Magnitude Average |B|\" #\n",
    "]\n",
    "\n",
    "filtered_data_omni.index = pd.date_range(str(min_date_year), str(max_date_year + 1), freq=\"60min\")[:-1]\n",
    "\n",
    "filtered_data_omni = filtered_data_omni[[\"Kp\", \"R\", \"DST\", \"Ap\", \"F10.7\", \"Proton temperature\", \"Proton density\", \"Plasma speed\", \"Alpha/Proton ratio\", \"Flow Pressure\", \"Field Magnitude Average |B|\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Explanation:\n",
    "1. The DataFrame `data` contains columns that need cleaning.\n",
    "2. We create a list of column names to process.\n",
    "3. For each column, we replace specific values with NaN using `np.where`.\n",
    "\n",
    "Note:\n",
    "- Adjust column names and replacement values according to your specific data.\n",
    "\"\"\"\n",
    "\n",
    "columns_to_clean = [\n",
    "    \"F10.7\",\n",
    "    \"Kp\",\n",
    "    \"R\",\n",
    "    \"DST\",\n",
    "    \"Ap\",\n",
    "    \"Proton temperature\",\n",
    "    \"Proton density\",\n",
    "    \"Plasma speed\",\n",
    "    \"Alpha/Proton ratio\",\n",
    "    \"Flow Pressure\",\n",
    "    \"Field Magnitude Average |B|\"\n",
    "]\n",
    "\n",
    "for col in columns_to_clean:\n",
    "    filtered_data_omni[col] = np.where(filtered_data_omni[col] == 999.9, np.nan,filtered_data_omni[col])\n",
    "    filtered_data_omni[col] = np.where(filtered_data_omni[col] == 99, np.nan, filtered_data_omni[col])\n",
    "    filtered_data_omni[col] = np.where(filtered_data_omni[col] == 999, np.nan, filtered_data_omni[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_omni = filtered_data_omni[(filtered_data_omni.index >= str(min_date_year))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_columns(df: pd.DataFrame, new_columns: typing.List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates new columns in a DataFrame and fills them with zeros.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to add columns to.\n",
    "        new_columns (typing.List[str]): A list of names for the new columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the new columns added.\n",
    "    \"\"\"\n",
    "\n",
    "    df[new_columns] = np.zeros((len(df), len(new_columns)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kp = create_columns(df=filtered_data_omni.copy(), new_columns=['G0', 'G1 ', 'G2', 'G3', 'G4', 'G5'])\n",
    "df_dst = create_columns(df=filtered_data_omni.copy(), new_columns=['Weak', 'Moderate', 'Intense', 'Severe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_kp: typing.Dict[str, pd.Series] = {\n",
    "    'G0': filtered_data_omni['Kp'].le(43),\n",
    "    'G1': filtered_data_omni['Kp'].between(46, 54),\n",
    "    'G2': filtered_data_omni['Kp'].between(56, 64),\n",
    "    'G3': filtered_data_omni['Kp'].between(66, 74),\n",
    "    'G4': filtered_data_omni['Kp'].between(76, 88),\n",
    "    'G5': filtered_data_omni['Kp'].ge(90)\n",
    "}\n",
    " \n",
    "for G, condition in conditions_kp.items():\n",
    "    df_kp[G] = condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_dst: typing.Dict[str, pd.Series] = {\n",
    "    'Weak': filtered_data_omni['DST'].between(-50, -30, inclusive='right'),\n",
    "    'Moderate': filtered_data_omni['DST'].between(-100, -50, inclusive='right'),\n",
    "    'Intense': filtered_data_omni['DST'].between(-200, -100, inclusive='right'),\n",
    "    'Severe': filtered_data_omni['DST'].le(-200)\n",
    "}\n",
    "\n",
    "for G, condition in conditions_dst.items():\n",
    "    df_dst[G] = condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_kp(df: pd.DataFrame, lvl, closed: bool = None) -> pd.DataFrame:\n",
    "    df_resampled_kp: pd.DataFrame = df.resample(rule=lvl, closed=closed).agg({\n",
    "        'Kp': 'max',\n",
    "        'R': 'mean',\n",
    "        'F10.7': 'mean',\n",
    "        'Proton temperature': 'mean',\n",
    "        'Proton density': 'mean',\n",
    "        'Plasma speed': 'mean',\n",
    "        'Alpha/Proton ratio': 'mean',\n",
    "        'Flow Pressure': 'mean',\n",
    "        'Field Magnitude Average |B|': 'mean',\n",
    "        'G1': 'sum',\n",
    "        'G1': 'sum',\n",
    "        'G2': 'sum',\n",
    "        'G3': 'sum',\n",
    "        'G4': 'sum',\n",
    "        'G5': 'sum'\n",
    "    })\n",
    "\n",
    "    df_resampled_kp['Total ST'] = df_resampled_kp[['G1', 'G2', 'G3', 'G4', 'G5']].sum(axis=1)\n",
    "    return df_resampled_kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_dst(df: pd.DataFrame, lvl, closed: bool = None) -> pd.DataFrame:\n",
    "    df_resampled_dst: pd.DataFrame = df.resample(rule=lvl, closed=closed).agg({\n",
    "        'DST': 'max',\n",
    "        'R': 'mean',\n",
    "        'F10.7': 'mean',\n",
    "        'Proton temperature': 'mean',\n",
    "        'Proton density': 'mean',\n",
    "        'Plasma speed': 'mean',\n",
    "        'Alpha/Proton ratio': 'mean',\n",
    "        'Flow Pressure': 'mean',\n",
    "        'Field Magnitude Average |B|': 'mean',\n",
    "        'Weak': 'sum',\n",
    "        'Moderate': 'sum',\n",
    "        'Intense': 'sum',\n",
    "        'Severe': 'sum'\n",
    "    })\n",
    "\n",
    "    df_resampled_dst['Total ST'] = df_resampled_dst[['Weak', 'Moderate', 'Intense', 'Severe']].sum(axis=1)\n",
    "    return df_resampled_dst\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq: str = \"ME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resample_kp: pd.DataFrame = resample_kp(df_kp, freq)\n",
    "df_resample_kp['Solar Cycle'] = pd.cut(df_resample_kp.index, \n",
    "                                        bins=[\n",
    "                                            pd.to_datetime('1964-10-01'),\n",
    "                                            pd.to_datetime('1976-03-01'),\n",
    "                                            pd.to_datetime('1986-09-01'),\n",
    "                                            pd.to_datetime(\"1996-08-01\"), \n",
    "                                            pd.to_datetime(\"2008-12-31\"),\n",
    "                                            pd.to_datetime(\"2019-12-31\"), \n",
    "                                            pd.to_datetime(\"2100-01-01\")\n",
    "                                        ],\n",
    "                                        labels=[20, 21, 22, 23, 24, 25])\n",
    "\n",
    "df_resample_kp = df_resample_kp.rename(columns={\"Kp\": \"Kp max\"})\n",
    "\n",
    "df_resample_dst: pd.DataFrame = resample_dst(df_dst, freq)\n",
    "df_resample_dst['Solar Cycle'] = pd.cut(df_resample_dst.index, \n",
    "                                        bins=[\n",
    "                                            pd.to_datetime('1964-10-01'),\n",
    "                                            pd.to_datetime('1976-03-01'),\n",
    "                                            pd.to_datetime('1986-09-01'),\n",
    "                                            pd.to_datetime(\"1996-08-01\"), \n",
    "                                            pd.to_datetime(\"2008-12-31\"),\n",
    "                                            pd.to_datetime(\"2019-12-31\"), \n",
    "                                            pd.to_datetime(\"2100-01-01\")\n",
    "                                        ],\n",
    "                                        labels=[20, 21, 22, 23, 24, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_columns(df: pd.DataFrame, columns_to_assign: typing.List[str]) -> pd.DataFrame:\n",
    "    for col in columns_to_assign:\n",
    "        df = df.assign(**{col.strip(): df_resample_kp[col.strip()]})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_CEC = assign_columns(df=df_resample_kp.copy(), columns_to_assign=['Total ST', 'G1', 'G2', 'G3', 'G4', 'G5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(c1_CEC['Total ST'], c1_CEC[field_to_analyze])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "(c1_CEC[field_to_analyze] / c1_CEC[field_to_analyze].max()).plot()\n",
    "(c1_CEC['Total ST'] / c1_CEC['Total ST'].max()).plot()\n",
    "plt.legend(ncol=3)\n",
    "plt.title(label=f\"Total Storms vs {field_to_analyze}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(np.array(c1_CEC[field_to_analyze]).astype(dtype=float), np.array(c1_CEC['Total ST']).astype(dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_CEC.corr(numeric_only=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
